# -*- coding: utf-8 -*-
"""loan_prediction_documentation copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AgHPMq7sv4nUTDkkOxwYq76E72ebUEBs

### Hello! Welcome to my portfolio, Iâ€™m Sals.

As a final project that will complete my journey through a virtual internship at Kalbe Nutritionals, I will develop predictive data models to improve the company's business such as optimizing business competitive strategies or creating regression and clustering analysis, then preparing visual media to present solutions to clients.

I invite you to explore my portfolio and review my work. As I believe in continuous learning and growth, I am open to any thoughts or recommendations you may have.

Feel free to connect and reach me on [Linked In](https://www.linkedin.com/in/salsabila-mardhiyah/)!

### Setup
"""

# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from scipy import stats
from scipy.stats import skew
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, cross_validate, learning_curve
from sklearn.preprocessing import StandardScaler, RobustScaler, Normalizer, MinMaxScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder, FunctionTransformer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import SelectKBest, f_classif

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, jaccard_score, log_loss, confusion_matrix, fbeta_score
from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import learning_curve, cross_val_score
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import get_scorer_names
# from skopt import BayesSearchCV

import shap

import warnings
warnings.filterwarnings('ignore')

"""### Importing Data"""

# Max rows and columns setting
pd.set_option('display.max_columns', None) # Display all columns
pd.set_option('display.max_rows', None) # Display all rows
pd.set_option('max_colwidth', None) # Display columns full width

# Importing dataset
df = pd.read_csv('data/loan_data_2007_2014.csv')
df.sample()

# Importing feature explaination
explanation = pd.read_csv('data/LCDataDictionary.xlsx - LoanStats.csv')
explanation.drop('Unnamed: 2',axis=1)

"""### 1. Data Exploration

#### 1.1 Descriptive Analysis

##### 1.1.1 Checking Data Shape
"""

# Defining numerical and categorical list of columns
num_col = df.describe().columns
cat_col = df.describe(exclude='number').columns

# Checking initial shape of data
print(f'Data cosists of {df.shape[0]} rows and {df.shape[1]} columns')
print(f'''({num_col.value_counts().sum()} numeric columns and {cat_col.value_counts().sum()} categorical columns)''')

"""##### 1.1.2 Checking Missing Data"""

# Checking data types and null values
df.info()

# Checking missing values percentage
missing = pd.DataFrame({'missing_values': df.isna().sum().sort_values(ascending=False),
                        'percentage': round(df.isna().sum().sort_values(ascending=False)/df.shape[0]*100)
                        })
missing

"""There are 17 features with 100% null value, therefore we will drop the columns"""

# Dropping columns with 100% null values
df.dropna(how='all', inplace=True, axis=1)

# Checking data shape
df.shape

"""##### 1.1.3 Checking Duplicated Data"""

# Checking duplicated values
df.duplicated().sum()

"""There is no duplicated values

##### 1.1.4 Removing Unnecessary Data
"""

# Defining numerical and categorical list of columns
num_col = df.describe().columns
cat_col = df.describe(exclude='number').columns

# Checking descriptive statistics for numerical columns
df.describe().T

# Checking unique values on numerical column
for col in num_col:
    print(f'{col} Unique Values: {df[col].unique().shape[0]}')
    print('-' * 30)

# Checking descriptive statistics for categorical columns
df.describe(exclude='number').T

# Checking some of categorical column values
some_of_cats = ['term', 'grade', 'emp_length', 'home_ownership', 'verification_status', 'loan_status', 'pymnt_plan', 'purpose', 'initial_list_status']
for cat in some_of_cats:
    print(f'{df[cat].value_counts()}')
    print('-' * 30)

"""These features are going to be dropped because of specific reason:
| Unnecessary Columns               | Causes                |
|-----------------------------------|-----------------------|
| `desc`                            | free text             |
| `url`, `id`, `member_id`          | all values are unique |
| `title`, `emp_title`, `addr_state`, `earliest_cr_line`, `zip_code`              | high cardinality      |
| `Unnamed: 0`                      | index                 |
| `policy_code`, `application_type` | one unique value      |
|  `sub_grade`, `int_rate` | redundant      |
| `pymnt_plan` | one dominant category       |
"""

# Make list of features about to drop
drop_cols = [
            'desc', 'url', 'id', 'zip_code', 'sub_grade',
            'member_id', 'title', 'emp_title', 'int_rate',
            'Unnamed: 0', 'policy_code', 'application_type',
            'pymnt_plan', 'earliest_cr_line', 'addr_state'
            ]

# Dropping features
df_dropped = df.drop(labels=drop_cols, axis=1, inplace=False)

# Checking df shape
df_dropped.shape

df_dropped.columns

"""In this project, we're building machine learning model with objective to classify wether a loan is risky and not risky before the loan can be accepted. Therefore, by selecting the features that appear after the loan accepted may indicate Data Leakage. As quoted from this [source](https://dataskeptic.com/blog/episodes/2016/leakage), "if any other feature whose value would not actually be available in practice at the time you'd want to use the model to make a prediction, is a feature that can introduce leakage to your model".

In this case, we will drop the features that indicates leakage from the future data and choose the available features adopted from the initial stage of loan application.
"""

# Make list of features indicates leakage from the future data
drop_cols = [
            'out_prncp',
            'out_prncp_inv',
            'total_pymnt',
            'total_pymnt_inv',
            'total_rec_prncp',
            'total_rec_int',
            'total_rec_late_fee',
            'recoveries',
            'collection_recovery_fee',
            'last_pymnt_d',
            'last_pymnt_amnt',
            'funded_amnt',
            'funded_amnt_inv',
            'issue_d',
            'last_credit_pull_d'
            ]

# Dropping features
df_dropped = df_dropped.drop(labels=drop_cols, axis=1, inplace=False)

# Checking df shape
df_dropped.shape

"""#### 1.2 Univariate Analysis

##### 1.2.1 Target Label

We are building machine learning to classify whether a loan is bad (risky/charged off) and good (not risky/fully paid) using the final status of the loan and not the ongoing loan because there is no certainty of the outcome of the ongoing loan. Therefore in this dataset, there are 9 types of loan status, namely:
1. Current
2. In Grace Period (Late 1-15 days)
3. Late (16-30 days)
4. Late (31-120 days)
5. Default (Late 131-150 days)
6. Charged Off (Late 150+ days)
7. Fully Paid
8. Does not meet the credit policy. Status:Fully Paid
    
    (While the loan was paid off, the loan application today would no longer meet the credit policy and wouldn't be approved on to the marketplace)
9. Does not meet the credit policy. Status:Charged Off

    (While the loan was charged off, the loan application today would no longer meet the credit policy and wouldn't be approved on to the marketplace)

The first until the 5th status indicates that the loan is still ongoing, While Fully Paid and Charged-Off means the final outcome of a loan. As quoted from [this](https://www.forbes.com/advisor/credit-cards/what-is-a-charge-off/) source, "a charge-off is a declaration by a creditor that an outstanding debt is unlikely to be collected".

While Default also has similar impact, it still ongoing and has small chance to be paid. So we only choose Charged-Off as the final outcome of a risky loan.
"""

# Checking value count of each loan status and percentage of data
status, count = df_dropped["loan_status"].value_counts().index, df_dropped["loan_status"].value_counts().values
percent = df_dropped["loan_status"].value_counts().values/df_dropped.shape[0]*100
pd.DataFrame({'Loan Status': status,'Count': count, 'Percentage':percent})[['Loan Status','Count', 'Percentage']]

"""Now we will simplify the loan status into a binary classification of Fully Paid or Charged loans by eliminating rows with other statuses. This implies that we must discard roughly 50% of our data, but let's emphasize that we are not interested in any statuses that show that the loan is ongoing or in progress, as presuming that anything is in progress doesn't tell us anything."""

# Selecting loan_status with Fully Paid or Charged Off values
df_two_class = df_dropped[(df_dropped['loan_status'] == 'Fully Paid') | (df_dropped['loan_status'] == 'Charged Off')]
df_two_class.sample(3)

"""Therefore, we can transform Charged Off values to 1 as bad loan and Fully Paid values to 0 as good loan."""

# Replace target values
mapping_loan = {'loan_status':{ 'Fully Paid': 0, 'Charged Off': 1}}
df_two_class = df_two_class.replace(mapping_loan)

# Checking value count of each loan status and percentage of data
status, count = df_two_class["loan_status"].value_counts().index, df_two_class["loan_status"].value_counts().values
percent = df_two_class["loan_status"].value_counts().values/df_two_class.shape[0]*100
pd.DataFrame({'Loan Status': status,'Count': count, 'Percentage':percent})[['Loan Status','Count', 'Percentage']]

"""From label count checking, there are 81% good loan and 19% bad loan. We will handle it with oversampling/undersampling method later."""

# Checking missing values percentage
missing = pd.DataFrame({'missing_values': df_two_class.isna().sum().sort_values(ascending=False),
                        'percentage': round(df_two_class.isna().sum().sort_values(ascending=False)/df_two_class.shape[0]*100)
                        })
missing

"""Our most recent data frame's missing values check revealed some features with more than 50% missing value. Avoiding adding bias when handling missing values, We shall thus remove those features and left with features that has missing values not more than 50%."""

# Make list of features have more then 50% missing values
drop_cols = [
            'next_pymnt_d',
            'mths_since_last_record',
            'mths_since_last_major_derog',
            'mths_since_last_delinq'
            ]

# Dropping features
df_two_class =df_two_class.drop(labels=drop_cols, axis=1, inplace=False)

# Checking df shape
df_two_class.shape

"""##### 1.2.2 Distribution of Features"""

# Defining numerical and categorical list of columns
num_col = df_two_class.describe().columns
cat_col = df_two_class.describe(exclude='number').columns

# histogram for numeric columns
fig, axes = plt.subplots(9, 2, figsize=(20, 15))

i = 0
for y in axes:
    try:
        for ax in y:
            column = num_col[i]
            sns.histplot(df_two_class, x=column, hue='loan_status', multiple='stack', ax=ax)
            ax.minorticks_on()
            i += 1
    except IndexError:
            pass
fig.tight_layout(pad=1)
plt.show()

"""We can deduce the following facts from the visualization above:
- Some of the features seems to have similar distribution. We have to analyze from correlation matrix if those features are highly correlated. If so, the two features become redundant features since they have same information and we have to eliminate one of them.
- According to the distribution, 'acc_now_delinq', 'collections_12_mths_ex_med','tot_coll_amt','pub_rec','delinq_2yrs' features can be grouped into discrete bins or categories  to simplify the data and reduce the impact of outliers.
"""

# boxplot for numeric columns
fig, axes = plt.subplots(9, 2, figsize=(20, 15))

i = 0
for y in axes:
    try:
        for ax in y:
            column = num_col[i]
            sns.boxplot(df_two_class, x=column, ax=ax)
            ax.minorticks_on()
            i += 1
    except IndexError:
            pass
fig.tight_layout(pad=1)
plt.show()

"""We can see from visualizatio above, there are many outliers in this data."""

# Create histplot for categorical columns
fig, axes = plt.subplots(4, 2, figsize=(40, 20))

i = 0
for y in axes:
    try:
        for ax in y:
            column = cat_col[i]
            sns.histplot(df_two_class, x=column, hue='loan_status', multiple='stack', ax=ax)
            i += 1
    except IndexError:
            pass

fig.tight_layout()
plt.show()

"""We can deduce the following facts from the visualization above:
- Most loans are taken out by people with 10 years or more of employment history
- The majority of loans are used to consolidate borrowers other debts  
- There is little difference between whole and fractional loans in terms of fully paid or charged off debt.

Several things that need to be followed up during data pre-processing:
   - For the `loan_status` column, mapping can be done so that the "Fully Paid" and "Charged Off" values become 0 and 1.
   - For the `home_ownership`, `grade`, `emp_length` column, label encoding can be done because the columns have sequential values and have an ordinal relationship between the values.
   - For the `term`, `initial_list_status`, `purpose`, and `verification_status` columns, one-hot encoding can be performed because these columns do not have an ordinal relationship between their values and each categorical value is considered equally important.

#### 1.3. Multivariate Analysis

##### 1.3.1 Correlation Heatmap
"""

# Correlation heatmap with spearman
plt.figure(figsize=(15,10))
sns.heatmap(df_two_class.corr(method='spearman', numeric_only=True), annot=True, cmap='BrBG', vmin=-1, vmax=1, fmt='.2f')

plt.title('Pearson Correlation for Numerical Feature', fontsize=14)
plt.show()

"""Based on the results of the heatmap, the correlation between various features is made in the range 1 to -1. The closer to 1 or -1, the stronger the correlation, while the closer to 0, the weaker the correlation. Some of the most relevant correlation values are as follows:
- `installment` and `loan_amnt` have very strong correlation
- `total_rev_hi_lim`, `loan_amnt`, `installment`, `annual_inc`, `open_acc`, `total_acc`, `revol_bal`, `tot_cur_bal` have moderate to very strong correlation on each other. If multicollinearity exists, features that have a lower correlation with the target (`loan_status`) can be removed or combined with other features.

### 2. Data Preprocessing

#### 2.1 Handling Missing Values

After dropping some of features, we're about to handle the remaining missing values with Imputation Method. Here is the missing value information from all remaining features:
"""

# Checking missing values percentage
missing = pd.DataFrame({'missing_values': df_two_class.isna().sum().sort_values(ascending=False),
                        'percentage': round(df_two_class.isna().sum().sort_values(ascending=False)/df_two_class.shape[0]*100)
                        })
missing

"""Now we are handling missing values with the Imputation Method, which imputes the mode value for categorical data and the median value for numerical data."""

# Define features that require mode imputation
mode_col = ['emp_length']

# Define features that require median imputation
median_col = [
            'revol_util',
            'collections_12_mths_ex_med',
            'tot_coll_amt',
            'tot_cur_bal',
            'total_rev_hi_lim'
            ]

# Copy data before imputation
df_imputed = df_two_class.copy()

# Perform mode imputation for mode_col
mode_imputer = SimpleImputer(strategy='most_frequent')
df_imputed[mode_col] = mode_imputer.fit_transform(df_imputed[mode_col])

# Perform median imputation for median_col
median_imputer = SimpleImputer(strategy='median')
df_imputed[median_col] = median_imputer.fit_transform(df_imputed[median_col])

# Checking missing values percentage
missing = pd.DataFrame({
                        'missing_values': df_imputed.isna().sum().sort_values(ascending=False),
                        'percentage': round(df_imputed.isna().sum().sort_values(ascending=False)/df_imputed.shape[0]*100)
                        })
missing

"""Re-checking missing values from the recent data frame, we now have no missing values in our data.

#### 2.2 Handling Outliers
"""

# # Defining numerical and categorical list of columns
# num_col = df_imputed.describe().columns
# cat_col = df_imputed.describe(exclude='number').columns

# # check for outliers from each numeric column using the z-score test
# zscore = pd.DataFrame()
# outlier = pd.DataFrame()
# filtered_zscore = np.array([True] * len(df_imputed))

# for col in num_col:
#     zscore = abs(stats.zscore(df_imputed[col])) # calculating absolute z-scorenya
#     outlier[col] = df_imputed[col][zscore>3]
#     filtered_zscore = (zscore < 3) & filtered_zscore

# outlier.info()

# # check for outliers from each numeric column using IQR
# outlier_iqr = pd.DataFrame()
# filtered_IQR = np.array([True] * len(df_imputed))

# for col in num_col:
#     Q1 = df_imputed[col].quantile(0.25)
#     Q3 = x_train[col].quantile(0.75)
#     IQR = Q3 - Q1
#     low_limit = Q1 - (IQR * 1.5)
#     high_limit = Q3 + (IQR * 1.5)
#     outlier_iqr[col] = df_imputed[col][(df[col] < low_limit) | (df[col] > high_limit)]
#     filtered_IQR = ((df_imputed[col] >= low_limit) & (df_imputed[col] <= high_limit)) & filtered_IQR

# outlier_iqr.info()

# # Dropping outlier using zscore
# print(f'Number of rows of train data before outlier filter: {len(df_imputed)}')

# # Get index of outliers
# outlier_id = outlier.index
# df_zscored = df_imputed.drop(outlier_id)

# print(f'Number of rows of train data before outlier filter: {len(df_categorized)}')
# print(f'Checks whether the number of rows x_train and y_train are the same: {len(df_categorized) == len(y_train_outlier_removed)}')

"""Using a z-score foundation for finding outliers, there are 2553 data from 14 features that are outliers (representing about 1.4% of the total data train), however using IQR Method, there are 5553 data from all features that are outliers (representing about 3% of the train data).

Outlier data can be removed if judged unneeded because there are not many outliers and a sizable amount of remaining data.

The z-score method was selected in this stage to identify and eliminate outlier data. Reduced from 181771 to 179218 data trains.

The box plot shows a large number of outliers in each feature. However, considering that financial datasets are complex with each data being most likely to be correct, on this occasion we will not deal with outlier problems.

Also since the dataset is complex with many features, we decided to use Decision Trees, Random Forests, and Gradient Boosting Algorithms which are generally robust to outliers in the data.

#### 2.3 Feature Extraction

Binning or categorization is a data preprocessing technique that involves grouping continuous or numerical data into discrete bins or categories. This technique is primarily used to simplify the data, reduce the impact of outliers, and make the data more manageable or suitable for specific types of analysis.
"""

# Features which will be categorized
num_cols_categorize = [
                    'acc_now_delinq',
                    'collections_12_mths_ex_med',
                    'tot_coll_amt',
                    'pub_rec',
                    'delinq_2yrs'
                    ]

# Checking descriptive statistics
df_imputed[num_cols_categorize].describe().T

# Checking unique values on numerical column
for col in num_cols_categorize:
    print(f'{col} Unique Values: {df_imputed[col].unique().shape[0]}')
    print('-' * 30)

# histogram for numeric columns
fig, axes = plt.subplots(3, 2, figsize=(20, 15))

i = 0
for y in axes:
    try:
        for ax in y:
            column = num_cols_categorize[i]
            sns.histplot(df_imputed, x=column, multiple='stack', ax=ax)
            ax.minorticks_on()
            i += 1
    except IndexError:
            pass
fig.tight_layout(pad=1)
plt.show()

"""Here we're going to use Custom Binning Discretization Methods, which allow us to define bins with specific boundaries based on domain knowledge or business requirements."""

# Create copy for data categorization
df_categorized = df_imputed.copy()
df_categorized.shape

"""1. **acc_now_delinq:**
- This feature represents the number of accounts on which the borrower is currently delinquent.
- Binning this feature into categories such as "No Delinquency" (0), "Low Delinquency" (1-2), and "High Delinquency" (3 or more) could be useful. This way, we capture different levels of delinquency.

"""

# categorize acc_now_delinq feature
condlist = [
            (df_categorized['acc_now_delinq'] == 0),
            (df_categorized['acc_now_delinq'] == 1) | (df_categorized['acc_now_delinq'] == 2),
            (df_categorized['acc_now_delinq'] >= 3)
            ]

choicelist = ['No Delinquency', 'Low Delinquency', 'High Delinquency']

df_categorized['acc_now_delinq'] = np.select(condlist, choicelist)

"""2. **collections_12_mths_ex_med:**
- This feature represents the number of collections in the last 12 months excluding medical collections.
- We can consider binning this feature into categories like "No Collections" (0) and "Collections Occurred" (1 or more) to distinguish between borrowers with and without recent collections.
"""

# categorize collections_12_mths_ex_med feature
condlist = [
            (df_categorized['collections_12_mths_ex_med'] == 0),
            (df_categorized['collections_12_mths_ex_med'] >= 1)
            ]

choicelist = ['No Collections', 'Collections Occurred']

df_categorized['collections_12_mths_ex_med'] = np.select(condlist, choicelist)

"""3. **tot_coll_amt:**
- This feature represents the total collection amount ever owed by the borrower.
- Given the wide range of values and the presence of outliers, we might want to consider creating bins based on meaningful thresholds. For example, "No Collections" (0), "Low Collections" (1-999), "Moderate Collections" (1,000-9,999), and "High Collections" (10,000 or more).

"""

# categorize tot_coll_amt feature
condlist = [
            (df_categorized['tot_coll_amt'] == 0),
            (df_categorized['tot_coll_amt'] >= 1) & (df_categorized['tot_coll_amt'] < 1000),
            (df_categorized['tot_coll_amt'] >= 1000) & (df_categorized['tot_coll_amt'] < 10000),
            (df_categorized['tot_coll_amt'] >= 10000)
            ]

choicelist = ['No Collections', 'Low Collections', 'Moderate Collections', 'High Collections']

df_categorized['tot_coll_amt'] = np.select(condlist, choicelist)

"""4. **pub_rec:**
- This feature represents the number of public record bankruptcies.
- Binning this feature into categories like "No Bankruptcy" (0) and "Bankruptcy Occurred" (1 or more) can help differentiate borrowers with and without bankruptcies.
"""

# categorize collections_12_mths_ex_med feature
condlist = [
            (df_categorized['pub_rec'] == 0),
            (df_categorized['pub_rec'] >= 1)
            ]

choicelist = ['No Bankruptcy', 'Bankruptcy Occurred']

df_categorized['pub_rec'] = np.select(condlist, choicelist)

"""5. **delinq_2yrs:**
- This feature represents the number of 30+ days past-due incidences in the last 2 years.
- We can consider creating bins such as "No Delinquencies" (0), "Low Delinquencies" (1-2), and "High Delinquencies" (3 or more) to capture varying levels of delinquency.
"""

# categorize tot_coll_amt feature
condlist = [
            (df_categorized['delinq_2yrs'] == 0),
            (df_categorized['delinq_2yrs'] >= 1) & (df_categorized['delinq_2yrs'] < 3),
            (df_categorized['delinq_2yrs'] >= 3)
            ]

choicelist = ['No Delinquencies', 'Low Delinquencies', 'High Delinquencies']

df_categorized['delinq_2yrs'] = np.select(condlist, choicelist)

"""After binning, we can convert these categorical bins into dummy variables (one-hot encoding)/label for use in machine learning models if needed.

#### 2.4 Feature Transformation
"""

# List of numerical columns that will be feature enegineered using categorization/binning
# num_cols_categorize = [
#                     'acc_now_delinq',
#                     'collections_12_mths_ex_med',
#                     'tot_coll_amt',
#                     'pub_rec',
#                     'delinq_2yrs'
#                     ]

# Defining numerical and categorical list of columns
num_col = df_categorized.describe().columns.difference(['loan_status'])
cat_col = df_categorized.describe(exclude='number').columns

num_col.shape

cat_col.shape

# Review the distribution of the numeric columns in the data using KDEplot of the smoothed data
fig, axes = plt.subplots(6, 2, figsize=(20, 15))
fig.suptitle('KDE Plot for Numerical Train Data', fontsize=16)

i = 0
for y in axes:
    try:
        for ax in y:
            column = num_col[i]
            sns.kdeplot(df_categorized, x=column, ax=ax)
            ax.minorticks_on()
            i += 1
    except IndexError:
            pass
fig.tight_layout(pad=1)
plt.show()

# Calculate the degree of skewness of each numerical data
skew_score = []
for i in num_col:
  skew_score.append(round(skew(df_categorized[i]),2))

df_skewness = pd.DataFrame({'Feature':num_col, 'Skewness':skew_score})
df_skewness.sort_values(by='Skewness')

"""From [this](https://www.analyticsvidhya.com/blog/2021/05/shape-of-data-skewness-and-kurtosis/) source, the general rules-of-thumb of skewness are:
- If skewness is less than -1 or greater than 1, the distribution is highly skewed.
- If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.
- If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.

From the skewness score generated above, we have information that:
- `revol_util` and `dti` distribution are approximateley symetric.
- `installment`, `total_acc`, and `loan_amnt` distribution are moderately right skewed.
- the remaining features have highly right skewed distribution.

The features that have moderately skewed and highly skewed distribution have to be transformed in order to get symetric distribution. [This](https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9) source explained various types of transformations of data to better fit for normal distribution.
"""

import numpy as np
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt

def boxcox_transform_and_plot(data):
    # Generate Box-Cox transformed data and get lambda value
    transformed_data, fitted_lambda = stats.boxcox(data)

    # Create subplots for original and transformed data
    fig, ax = plt.subplots(1, 2, figsize=(10, 5))

    # Plot original data
    sns.distplot(data, hist=False, kde=True,
                 kde_kws={'shade': True, 'linewidth': 2},
                 label='Original Data', color='green', ax=ax[0])

    # Plot transformed data
    sns.distplot(transformed_data, hist=False, kde=True,
                 kde_kws={'shade': True, 'linewidth': 2},
                 label='Transformed Data', color='blue', ax=ax[1])

    # Add legends
    ax[0].legend(loc='upper right')
    ax[1].legend(loc='upper right')

    # Set plot labels
    ax[0].set_title('Original Data Distribution')
    ax[1].set_title('Box-Cox Transformed Data Distribution')

    # Show the plots
    plt.show()

    # Return the lambda value used for transformation
    return transformed_data, fitted_lambda

# Transforming skewed with box-cox method

# Defining numerical list of columns except symtric features
num_col = df_categorized.describe().columns.difference(['revol_util', 'dti','loan_status'])

# Copy dataset
df_transformed = df_categorized.copy()

# Create list of transformed columns
transformed_num_cols = []

for col in num_col:
    try:
        original_data = df_categorized[col]
        transformed_data, lambda_value = boxcox_transform_and_plot(original_data)

        # Replace the original data with the transformed data
        df_transformed[col] = transformed_data

        # Print lambda value
        print(f"Lambda value used for Transformation of {col}: {lambda_value}")

        # Add transformed column to list
        transformed_num_cols.append(col)

    except ValueError:
        pass

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

def log_transform_and_plot(data):
    # Apply log transformation to the data
    transformed_data = np.log1p(data)  # Using np.log1p to handle zero and negative values

    # Create subplots for original and transformed data
    fig, ax = plt.subplots(1, 2, figsize=(10, 5))

    # Plot original data
    sns.distplot(data, hist=False, kde=True,
                 kde_kws={'shade': True, 'linewidth': 2},
                 label='Original Data', color='green', ax=ax[0])

    # Plot transformed data
    sns.distplot(transformed_data, hist=False, kde=True,
                 kde_kws={'shade': True, 'linewidth': 2},
                 label='Transformed Data', color='blue', ax=ax[1])

    # Add legends
    ax[0].legend(loc='upper right')
    ax[1].legend(loc='upper right')

    # Set plot labels
    ax[0].set_title('Original Data Distribution')
    ax[1].set_title('Log Transformed Data Distribution')

    # Show the plots
    plt.show()

    return transformed_data

# Deleting column that has transformed from column list
num_col = num_col.difference(transformed_num_cols)

for col in num_col:
    try:
        original_data = df_categorized[col]
        transformed_data = log_transform_and_plot(original_data)

        # Replace the original data with the transformed data
        df_transformed[col] = transformed_data

        # Add transformed column to list
        transformed_num_cols.append(col)
    except ValueError:
        continue

# Defining numerical and categorical list of columns
num_col = df_categorized.describe().columns.difference(['loan_status'])

# Calculate the degree of skewness before and after transformation for each numerical data
skew_score = []
transformed_skew_score = []

for i in num_col:
  skew_score.append(round(skew(df_categorized[i]),2))
  transformed_skew_score.append(round(skew(df_transformed[i]),2))
df_skewness = pd.DataFrame({'Feature':num_col, 'Skewness':skew_score, 'Transformed Skewness':transformed_skew_score})
df_skewness.sort_values(by='Transformed Skewness')

"""After conducting transformation with

"""

still_skewed_col = [
                    'inq_last_6mths',
                    'tot_cur_bal',
                    'total_rev_hi_lim',
                    'revol_bal'
                    ]

import numpy as np
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt

def yeojohnson_transform_and_plot(data):
    # Generate Yeo-Johnson transformed data and get optimal lambdas
    transformed_data, fitted_lambda = stats.yeojohnson(data)

    # Create subplots for original and transformed data
    fig, ax = plt.subplots(1, 2, figsize=(10, 5))

    # Plot original data
    sns.distplot(data, hist=False, kde=True,
                 kde_kws={'shade': True, 'linewidth': 2},
                 label='Original Data', color='green', ax=ax[0])

    # Plot transformed data
    sns.distplot(transformed_data, hist=False, kde=True,
                 kde_kws={'shade': True, 'linewidth': 2},
                 label='Transformed Data', color='blue', ax=ax[1])

    # Add legends
    ax[0].legend(loc='upper right')
    ax[1].legend(loc='upper right')

    # Set plot labels
    ax[0].set_title('Original Data Distribution')
    ax[1].set_title('Yeo-Johnson Transformed Data Distribution')

    # Show the plots
    plt.show()

    # Return the fitted lambdas
    return transformed_data, fitted_lambda

# Transforming skewed with yeojohnson method

# Create list of transformed columns
transformed_num_cols = []

for col in still_skewed_col:
    try:
        original_data = df_categorized[col]
        transformed_data, lambda_value = yeojohnson_transform_and_plot(original_data)

        # Replace the original data with the transformed data
        df_transformed[col] = transformed_data

        # Print lambda value
        print(f"Lambda value used for Transformation of {col}: {lambda_value}")

        # Add transformed column to list
        transformed_num_cols.append(col)

    except ValueError:
        pass

# Defining numerical and categorical list of columns
num_col = df_categorized.describe().columns.difference(['loan_status'])

# Calculate the degree of skewness before and after transformation for each numerical data
skew_score = []
transformed_skew_score = []

for i in num_col:
  skew_score.append(round(skew(df_categorized[i]),2))
  transformed_skew_score.append(round(skew(df_transformed[i]),2))
df_skewness = pd.DataFrame({'Feature':num_col, 'Skewness':skew_score, 'Transformed Skewness':transformed_skew_score})
df_skewness.sort_values(by='Transformed Skewness')

"""All features' |skewness scores| are now less than 0.5, thus the distribution can be approximately symmetric and considered a normal distribution."""

df_transformed.shape

# # List of numerical columns that will be feature engineered using categorization/binning
# num_cols_categorize = [
#                     'acc_now_delinq',
#                     'collections_12_mths_ex_med',
#                     'tot_coll_amt',
#                     'pub_rec',
#                     'delinq_2yrs'
#                     ]

# Defining numerical and categorical list of columns
num_col = df_categorized.describe().columns.difference(['loan_status'])
cat_col = df_categorized.describe(exclude='number').columns

num_col.shape

cat_col.shape

df_transformed[num_col] = MinMaxScaler().fit_transform(df_transformed[num_col].values.reshape(df_categorized[num_col].shape))

# KDE Plot for Numerical Train Data After Transformation
fig, axes = plt.subplots(6, 2, figsize=(15, 8))

i = 0
for y in axes:
    try:
        for ax in y:
            column = num_col[i]
            sns.kdeplot(df_transformed, x=column, ax=ax)
            ax.minorticks_on()
            i += 1
    except IndexError:
            pass

fig.suptitle('KDE Plot for Numerical Train Data After Transformation and Scaling', fontsize=16)
fig.tight_layout(pad=1)
plt.show()

# Checking numeric features stats
df_transformed[num_col].describe().T

"""Based on examination of the plot and descriptive statistics, it can be seen that trasformation and standardization have been successfully carried out, where the final data distribution for each feature is in the range 0 to 1.

The same transformation will be carried out on the test data as well so that the model formed can be applied to the test data as well.

#### 2.5 Feature Encoding

There are several categorical features whose data type is in the form of an object, so before entering them into the model, it is necessary to encode them into one or more columns in numerical form for these features so that the model can be more easily formed.
"""

# Create copy before encoding
df_encoded = df_transformed.copy()

"""Below we conduct Label Encoding to features that have ordinal values. The ordinal categories represent a clear and meaningful order or hierarchy based on creditworthiness and commitment to financial obligations."""

# Encoding 'grade' feature
grade_dict = {'B': 2, 'C': 3, 'D': 4, 'A': 1, 'E': 5, 'F': 6, 'G': 7}

# convert grade value to integer A to 1 until G to 7
df_encoded['grade'] = df_encoded['grade'].map(grade_dict)

# Encoding 'emp_length' feature
emp_length_dict = {
                '2 years': 0,
                '10+ years': 1,
                '9 years': 2,
                '8 years': 3,
                '6 years': 4,
                '< 1 year': 5,
                '1 year': 6,
                '7 years': 7,
                '5 years': 8,
                '3 years': 9,
                '4 years': 10
                }
df_encoded['emp_length'] = df_encoded['emp_length'].map(emp_length_dict)

# Encoding 'delinq_2yrs' feature
delinq_2yrs_dict = {
                    'No Delinquencies':2,
                    'Low Delinquencies':1,
                    'High Delinquencies':0
                    }
df_encoded['delinq_2yrs'] = df_encoded['delinq_2yrs'].map(delinq_2yrs_dict)

# Encoding 'pub_rec' feature
pub_rec_dict = {
                'No Bankruptcy': 1,
                'Bankruptcy Occurred': 0
                }
df_encoded['pub_rec'] = df_encoded['pub_rec'].map(pub_rec_dict)

# Encoding 'collections_12_mths_ex_med' feature
collections_12_mths_ex_med_dict = {
                                    'No Collections': 1,
                                    'Collections Occurred': 0
                                    }
df_encoded['collections_12_mths_ex_med'] = df_encoded['collections_12_mths_ex_med'].map(collections_12_mths_ex_med_dict)

# Encoding 'acc_now_delinq' feature
acc_now_delinq_dict = {
                        'No Delinquency': 2,
                        'Low Delinquency': 1,
                        'High Delinquency': 0
                        }
df_encoded['acc_now_delinq'] = df_encoded['acc_now_delinq'].map(acc_now_delinq_dict)

# Encoding 'tot_coll_amt' feature
tot_coll_amt_dict = {
                    'No Collections': 3,
                    'Low Collections': 2,
                    'Moderate Collections': 1,
                    'High Collections': 0
                    }
df_encoded['tot_coll_amt'] = df_encoded['tot_coll_amt'].map(tot_coll_amt_dict)

# Encoding 'home_ownership' feature
home_ownership_dict = {
                        'OWN': 3,
                        'MORTGAGE': 2,
                        'RENT': 1,
                        'OTHER': 0,
                        'NONE': 0,
                        'ANY':0
                        }
df_encoded['home_ownership'] = df_encoded['home_ownership'].map(home_ownership_dict)

# Encoding 'verification_status' feature
verification_status_dict = {
                            'Not Verified': 0,
                            'Verified': 1,
                            'Source Verified': 2
                            }
df_encoded['verification_status'] = df_encoded['verification_status'].map(verification_status_dict)

"""Below we conduct feature encoding with One Hot Encoding Method. This method is used to encode categorical features that considered nominal values."""

# Create list of OHE features
ohe_list = ['purpose', 'initial_list_status', 'term']

# Create OHE from list of features
dummy_df = pd.get_dummies(df_encoded[ohe_list])

# Concatinate OHE to dataframe
df_encoded = pd.concat([df_encoded, dummy_df], axis=1)

# Dropping initial features
df_encoded = df_encoded.drop(ohe_list, axis=1)

# Checking encoded dataframe info
df_encoded.info()

df_encoded.describe()

"""### 3. Machine Learning Modelling & Evaluation

#### 3.1 Data Train-Test Split

The data is divided into train and test data, before being processed further. The model will be created using train data, and it will be tested using test data.
"""

# Splitting data into features & target
y = df_encoded['loan_status']
X = df_encoded.drop(labels='loan_status', axis=1, inplace=False)

print(X.shape)
print(y.shape)

# Splittig train and test with 80:20 ratio
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101, stratify=y)

# Check amount of data on train and test
print('Train Data Shape =', x_train.shape[0])
print('Test Data Shape =', x_test.shape[0])

# Check if split is balanced based on target value
print('mean value of y on train =', y_train.mean())
print('mean value of y on test =', y_test.mean())

"""#### 3.2 Feature Selection

##### 3.2.1 Correlation Heat Map
"""

# copy encoded dataframe and add y_train back for feature heatmap
df_heatmap = df_encoded.copy()
df_heatmap.sample()

# Correlation heatmap
plt.figure(figsize=(20, 20))
sns.heatmap(df_heatmap.corr(method='spearman', numeric_only=True), cmap='BrBG', annot=True, vmin=-1, vmax=1, fmt='.2f')

plt.title('Correlation Heatmap (Train Data)')
plt.show()

"""From the correlatio heatmap above, we can conclude that:
- `installment` and `loan_amnt` have 0.97 correlation score, therefore we're about to drop the `installment` feature because it less correlated with the target.
- `term_36 months` and `term_60 months` are perfectly correlated to each other. but because they are One Hot Encoded, we decided not to drop it and letting anoother feature selection analyze first.
"""

x_train.shape

x_test.shape

# Dropping `installlment` feature on train and test data
x_train = x_train.drop(labels='installment', axis=1, inplace=False)
x_test = x_test.drop(labels='installment', axis=1, inplace=False)

# Re checking train and test data shape
print(x_train.shape)
print(x_test.shape)

"""##### 3.2.2 ANOVA"""

# using ANOVA
selector = SelectKBest(f_classif, k='all')
kbest_fit = selector.fit_transform(x_train, y_train)

columns = x_train.columns

feature_scores = list(zip(selector.scores_,columns))
sorted_feature_scores = sorted(feature_scores,reverse=True)

num_list = []
col_list = []
for i in range(len(columns)):
   num_list.append((sorted_feature_scores[i])[0])
   col_list.append((sorted_feature_scores[i])[1])

plt.bar(col_list,num_list)
plt.xticks(rotation=90)
plt.title('Ranking of Feature Importance Based on ANOVA')

plt.show()

pd.DataFrame({'feature': col_list, 'importance': num_list})

"""##### 3.2.3 Recursive Feature Elimination (RFE)"""

from sklearn.feature_selection import RFE
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier  # You can use any classifier of your choice
import numpy as np

def perform_rfe(X, y, num_features_to_select, model):
    """
    Perform Recursive Feature Elimination (RFE) to select a specified number of features.

    Parameters:
    - X: Feature matrix (numpy array or DataFrame)
    - y: Target vector (numpy array or Series)
    - num_features_to_select: Number of features to select
    - model: The estimator used for RFE (e.g., a classifier or regressor)

    Returns:
    - selected_features: List of selected feature indices
    """

    # Initialize RFE without specifying the estimator
    rfe = RFE(estimator=None, n_features_to_select=num_features_to_select)

    # Set the estimator (model) for RFE
    rfe.estimator = model

    # Fit RFE to the data
    rfe.fit(X, y)

    # Get the indices of selected features
    selected_features = np.where(rfe.support_)[0]

    return selected_features

# Example usage:
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# clf = RandomForestClassifier()  # You can use any classifier of your choice
# selected_features = perform_rfe(X_train, y_train, num_features_to_select=20, model=clf)
# print("Selected feature indices:", selected_features)

# RFE with num_features_to_select = 20
clf = RandomForestClassifier()  # You can use any classifier of your choice
selected_features = perform_rfe(x_train, y_train, num_features_to_select=20, model=clf)

# Get the names of the selected features
selected_feature_names = x_train.columns[selected_features].tolist()
print("Selected feature names:", selected_feature_names)

# Get the names of the selected features
selected_feature_names = x_train.columns[selected_features].tolist()
print("Selected feature names:", selected_feature_names)

top_20 = [
            'loan_amnt', 'grade', 'emp_length',
            'home_ownership', 'annual_inc',
            'verification_status', 'dti',
            'delinq_2yrs', 'inq_last_6mths',
            'open_acc', 'revol_bal', 'revol_util',
            'total_acc', 'tot_coll_amt', 'tot_cur_bal',
            'total_rev_hi_lim', 'purpose_debt_consolidation',
            'initial_list_status_w', 'term_ 36 months',
            'term_ 60 months'
            ]

# Define columns about to drop
top_20_diff = x_train.columns.difference(top_20)

# Dropping features
x_train_20 = x_train.drop(labels= top_20_diff, axis=1, inplace=False)
x_test_20 = x_test.drop(labels= top_20_diff, axis=1, inplace=False)

# Re checking train and test shape
print(x_train_20.shape)
print(x_test_20.shape)
# num_col = df_categorized.describe().columns.difference(['loan_status'])

"""#### 3.3 Handle Imbalance Class

The distribution of the fully paid loan ratio in train data and test data is similar because stratification has been carried out based on targets at the data separation stage, and removing outliers does not have a significant impact on this ratio because the number of outliers is not too significant and is also estimated to be evenly distributed. A ratio that is close to 80:20 is considered imbalance so action needs to be taken to balance the ratio.
"""

# check ratio on Fully Paid vs Charged Off Loans
plt.subplot(1, 2, 1)
plt.title('Y_train Ratio')
y_train.value_counts().plot.pie(labels=['Fully Paid', 'Charged Off'], colors=['tab:orange', 'tab:blue'], autopct='%.2f%%')
plt.ylabel('')

plt.subplot(1, 2, 2)
plt.title('Y_test Ratio')
y_test.value_counts().plot.pie(labels=['Fully Paid', 'Charged Off'], colors=['tab:orange', 'tab:blue'], autopct='%.2f%%')
plt.ylabel('')

# Add a title to the whole plot
fig.suptitle('Ratio of Fully Paid and Charged Off Loans for Train and Test Data', fontsize=16, fontweight='bold', y=1.1)

plt.tight_layout(pad=1)
plt.show()

"""On this occasion, we will use an oversampling method, namely the Synthetic Minority Over-sampling Technique (SMOTE). This is because keeping as much information as possible from the majority group is important, as we don't want to overlook potentially creditworthy applicants. Oversampling can help address the problem of class imbalance while preserving the wealth of the majority class."""

# Checking value count of each loan status and percentage of data
status, count = y_train.value_counts().index, y_train.value_counts().values
percent = y_train.value_counts().values/y_train.shape[0]*100
pd.DataFrame({'Loan Status': status,'Count': count, 'Percentage':percent})[['Loan Status','Count', 'Percentage']]

"""When we evaluate a trained model, it is important to use original, unmodified test data. This allows us to assess how well the model performs on imbalanced real-world data. Meanwhile, using the same handling imbalance data technique on test data can lead to unrealistic or overly optimistic evaluation results. So we will only handle imbalance on training data.

Here we're using one of oversampling technique SMOTE, where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling.
"""

# Checking value count of each loan status and percentage of data
status, count =y_test.value_counts().index,y_test.value_counts().values
percent =y_test.value_counts().values/y_test.shape[0]*100
pd.DataFrame({'Loan Status': status,'Count': count, 'Percentage':percent})[['Loan Status','Count', 'Percentage']]

# Handle Imbalance Class on Training Data Top 20
from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=101)
x_train_20_res, y_train_20_res = sm.fit_resample(x_train_20, y_train)

print('After OverSampling, the shape of train_X: {}'.format(x_train_20_res.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(y_train_20_res.shape))

print("After OverSampling, counts of label '1': {}".format(sum(y_train_20_res == 1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_train_20_res == 0)))

# check ratio on Fully Paid vs Charged Off Loans After Oversampling
plt.subplot(1, 2, 1)
plt.title('Y_train Ratio')
y_train_20_res.value_counts().plot.pie(labels=['Fully Paid', 'Charged Off'], colors=['tab:orange', 'tab:blue'], autopct='%.2f%%')
plt.ylabel('')

plt.subplot(1, 2, 2)
plt.title('Y_test Ratio')
y_test.value_counts().plot.pie(labels=['Fully Paid', 'Charged Off'], colors=['tab:orange', 'tab:blue'], autopct='%.2f%%')
plt.ylabel('')

# Add a title to the whole plot
fig.suptitle('Ratio of Fully Paid and Charged Off Loans for Train and Test Data', fontsize=16, fontweight='bold', y=1.1)

plt.tight_layout(pad=1)
plt.show()

"""We now have balance class on training data.

#### 3.4 Define Function
"""

def create_model(X_train, X_test, y_train, y_test, models):
    performance = []

    ensemble = VotingClassifier(estimators=[(str(i), i) for i in models], voting='hard')

    for model in models:
        # Hyperparameter tuning using RandomizedSearchCV
        param_dist = {}  # Define hyperparameters for tuning
        random_search = RandomizedSearchCV(model, param_dist, cv=2, n_iter=3)
        random_search.fit(X_train, y_train)
        model = random_search.best_estimator_

        model.fit(X_train, y_train)
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)

        # Update performance evaluation using the selected features
        train_accuracy, test_accuracy = accuracy_score(y_train, y_pred_train), accuracy_score(y_test, y_pred_test)
        train_precision, test_precision = precision_score(y_train, y_pred_train, pos_label=0), precision_score(y_test, y_pred_test, pos_label=0)
        train_recall, test_recall = recall_score(y_train, y_pred_train, pos_label=0), recall_score(y_test, y_pred_test, pos_label=0)
        train_f1, test_f1 = f1_score(y_train, y_pred_train, pos_label=0), f1_score(y_test, y_pred_test, pos_label=0)
        train_f2, test_f2 = fbeta_score(y_train, y_pred_train, pos_label=0, beta=2), fbeta_score(y_test, y_pred_test, pos_label=0, beta=2)
        performance.append([train_accuracy, test_accuracy,
                            train_precision, test_precision,
                            train_recall, test_recall,
                            train_f1, test_f1,
                            train_f2, test_f2])

    # Ensemble model predictions
    ensemble.fit(X_train, y_train)
    y_pred_train_ensemble = ensemble.predict(X_train)
    y_pred_test_ensemble = ensemble.predict(X_test)

    # Update performance evaluation with ensemble predictions
    train_accuracy, test_accuracy = accuracy_score(y_train, y_pred_train_ensemble), accuracy_score(y_test, y_pred_test_ensemble)
    train_precision, test_precision = precision_score(y_train, y_pred_train_ensemble, pos_label=0), precision_score(y_test, y_pred_test_ensemble, pos_label=0)
    train_recall, test_recall = recall_score(y_train, y_pred_train_ensemble, pos_label=0), recall_score(y_test, y_pred_test_ensemble, pos_label=0)
    train_f1, test_f1 = f1_score(y_train, y_pred_train_ensemble, pos_label=0), f1_score(y_test, y_pred_test_ensemble, pos_label=0)
    train_f2, test_f2 = fbeta_score(y_train, y_pred_train_ensemble, pos_label=0, beta=2), fbeta_score(y_test, y_pred_test_ensemble, pos_label=0, beta=2)
    performance.append([train_accuracy, test_accuracy,
                        train_precision, test_precision,
                        train_recall, test_recall,
                        train_f1, test_f1,
                        train_f2, test_f2])

    performance_df = pd.DataFrame(data=performance,
                                  columns='Train\nAccuracy, Test\nAccuracy, Train\nPrecision, Test\nPrecision, Train\nRecall, Test\nRecall, Train\nF1 Score, Test\nF1 Score, Train\nF2 Score, Test\nF2 Score'.split(','),
                                  index=[str(model).split('(')[0] for model in models] + ['Ensemble'])

    plt.figure(figsize=(12, 9))
    sns.heatmap(performance_df, cmap='mako', annot=True, annot_kws={'fontsize': 10})
    plt.tick_params(axis='both',
                    which='major',
                    labelsize=10,
                    labelbottom=False, bottom=False,
                    labeltop=True, top=False)
    plt.tight_layout()
    plt.savefig('modelling.png')
    plt.show()

# Define c_matrix_plot() function
def c_matrix_plot(y_test, y_pred):

    c_matrix = confusion_matrix(y_test, y_pred)
    names = ['True Negative', 'False Positive', 'False Negative', 'True Positive']
    counts = ['{0:0.0f}'.format(value) for value in c_matrix.flatten()]
    percentages = ['{0:.2%}'.format(value) for value in c_matrix.flatten() / np.sum(c_matrix)]
    labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names, counts, percentages)]
    labels = np.asarray(labels).reshape(2, 2)

    ax = sns.heatmap(c_matrix, annot = labels, fmt = '', cmap = 'mako')
    ax.set_xlabel('\nPredicted Values')
    ax.set_ylabel('Actual Values ');
    ax.xaxis.set_ticklabels(['non-default loan', 'default loan'])
    ax.yaxis.set_ticklabels(['non-default loan', 'default loan'])
    plt.show()


# Define plot_learning_curve() function
def plot_learning_curve(model, X, y, cv, train_sizes):
    train_sizes, train_scores, test_scores = learning_curve(
        model, X, y, cv=cv, train_sizes=train_sizes, scoring='accuracy'
    )
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    plt.figure(figsize=(8, 6))
    plt.title('Learning Curve')
    plt.xlabel('Training Examples')
    plt.ylabel('Score')
    plt.grid(True)
    plt.fill_between(
        train_sizes, train_scores_mean - train_scores_std,
        train_scores_mean + train_scores_std, alpha=0.1, color='r'
    )
    plt.fill_between(
        train_sizes, test_scores_mean - test_scores_std,
        test_scores_mean + test_scores_std, alpha=0.1, color='g'
    )
    plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training Score')
    plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-Validation Score')
    plt.legend(loc='best')
    plt.tight_layout()
    plt.show()

random_state = 101
models = [
    ['Decision Tree', DecisionTreeClassifier(random_state=random_state)],
    ['Random Forest', RandomForestClassifier(random_state=random_state)],
    ['Gradient Boosting', GradientBoostingClassifier(random_state=random_state)],
    ['XGBoost', XGBClassifier(random_state=random_state)],
    ['AdaBoost', AdaBoostClassifier(random_state=random_state)]
]

def cross_validation(X_train, y_train, model):
    model_name = []

    cv_accuracy_mean = []
    cv_accuracy_std = []
    training_accuracy = []

    cv_precision_mean = []
    cv_precision_std = []
    training_precision = []

    cv_recall_mean = []
    cv_recall_std = []
    training_recall = []

    cv_f1_mean = []
    cv_f1_std = []
    training_f1 = []
    for name, model in models:

        pipeline = Pipeline([
            ('model', model)
        ])
        model_name.append(name)

        #scoring
        scoring= ['accuracy', 'precision', 'recall', 'f1']

        # test
        cv_score = cross_validate(pipeline, X_train, y_train, scoring=scoring, cv=5, n_jobs=-1)

        cv_accuracy_mean.append(cv_score['test_accuracy'].mean())
        cv_accuracy_std.append(cv_score['test_accuracy'].std())

        cv_precision_mean.append(abs(cv_score['test_precision']).mean())
        cv_precision_std.append(abs(cv_score['test_precision']).std())

        cv_recall_mean.append(abs(cv_score['test_recall']).mean())
        cv_recall_std.append(abs(cv_score['test_recall']).std())

        cv_f1_mean.append(abs(cv_score['test_f1']).mean())
        cv_f1_std.append(abs(cv_score['test_f1']).std())

        # training
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_train)
        training_accuracy.append(accuracy_score(y_train, y_pred))
        training_precision.append(precision_score(y_train, y_pred))
        training_recall.append(recall_score(y_train, y_pred))
        training_f1.append(f1_score(y_train, y_pred))

    return pd.DataFrame({
        'Model': model_name,
        'Training Accuracy': training_accuracy,
        'CV Accuracy (mean)': cv_accuracy_mean,
        'CV Accuracy (std)' : cv_accuracy_std,
        'Training Precision' : training_precision,
        'CV Precision (mean)': cv_precision_mean,
        'CV Precision (std)': cv_precision_std,
        'Training Recall' : training_recall,
        'CV Recall (mean)' : cv_recall_mean,
        'CV Recall (std)' : cv_recall_std,
        'Training F1 Score' : training_f1,
        'CV F1 Score (mean)' : cv_f1_mean,
        'CV F1 Score (std)' : cv_f1_std
    })

"""#### 3.5 Model Evaluation

**F1 Score**

The F1 score is the harmonic mean of precision and recall, given by the formula:

F1 = 2 * (precision * recall) / (precision + recall).


It provides a balanced measure of precision and recall and is suitable when we want to strike a balance between minimizing false positives (precision) and false negatives (recall).
The F1 score gives equal weight to both false positives and false negatives, making it a balanced metric.

Rejecting a creditworthy applicant (false negative) could lead to a loss of potential business and customer dissatisfaction. Approving a high-risk applicant (false positive) can result in financial losses due to loan defaults and increased credit risk.

We would aim for a balanced approach between precision and recall. So the F1 score, which balances false positives and false negatives equally, could be a suitable evaluation metric.

#### 3.6 Model Evaluation: Cross-Validation

The F1 score from training is based on how well each model fits the training data. An F1 score of 1.0 or close to 1.0 suggests that the model has almost perfectly fit the training data, which can be a sign of overfitting.

Cross-validation provides a more realistic estimate of a model's performance on unseen data by evaluating it on multiple subsets of the data. The mean CV F1 score represents the model's average performance across these subsets, while the standard deviation indicates how much the performance varies.
"""

# SMOTE cross-val Top 20
cross_validation(x_train_20_res, y_train_20_res, models)

"""While the Decision Tree and Random Forest have high F1 scores from training, they also have relatively high standard deviations in CV F1 scores. This suggests that they may suffer from overfitting and exhibit variability in performance across different data splits.

The XGBoost model has a relatively high mean CV F1 score with a substantial standard deviation, indicating some variability.

The AdaBoost model have more stable performance with lower standard deviations, which is desirable for model consistency.

Then we try to build two hyperparameter tuned models, Adaboost-which has lowest standard deviation showing more stable performance and Random Forest-which shows highest mean cross validation F1-Score.

##### 3.6.1 AdaBoost Classifier Tuning

Hyperparameter tuning in **AdaBoost Classifier** is very important to improve model performance. With hyperparameter tuning we can find the optimal hyperparameter combination and can improve model performance significantly.
"""

abc = AdaBoostClassifier()

# define the grid of values to search
grid = dict()
grid['n_estimators'] = [10, 50, 100, 500]
grid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]

# define the grid search procedure
grid_search = GridSearchCV(estimator=abc, param_grid=grid, n_jobs=-1, cv=5, scoring='f1')

# execute the grid search
grid_result = grid_search.fit(x_train_20_res, y_train_20_res)

# summarize the best score and configuration
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

# input best_params_ to new classifier
b_abc = AdaBoostClassifier(
                        random_state=random_state, n_estimators= 500,
                        learning_rate= 1.0
                        )

# fit pipeline to data train
b_abc.fit(x_train_20_res, y_train_20_res)

# evaluate model
y_train_pred = b_abc.predict(x_train_20_res)
y_pred = b_abc.predict(x_test_20)

# main model evaluation: f1 score
print('f1 score data TRAIN :', round(f1_score(y_train_20_res, y_train_pred)*100, 3), '%')
print('f1 score data TEST  :', round(f1_score(y_test, y_pred)*100, 3), '%')

# other scores
print('\nOther Scores (based on test data):\n')
print('Accuracy score :', round(accuracy_score(y_test, y_pred)*100, 3), '%')
print('recall score :', round(recall_score(y_test, y_pred)*100, 3), '%')
print('Precision score :', round(precision_score(y_test, y_pred)*100, 3), '%')
print('Jaccard score :', round(jaccard_score(y_test, y_pred)*100, 3), '%')

# mengukur perbedaan antara probabilitas aktual dan probabilitas prediksi yang dihitung sebagai logaritma basis 2 dari likelihood
print('Log Loss :', (log_loss(y_test, y_pred)))

from sklearn.metrics._classification import classification_report

target_names = ['bad_loan', 'good_loan']

print(classification_report(y_test, y_pred, digits=3, target_names= target_names))

c_matrix_plot(y_test, y_pred)

# Compute the learning curve
train_sizes, train_scores, test_scores = learning_curve(
    b_abc, x_train_20_res, y_train_20_res, cv=5, scoring='f1', train_sizes=np.linspace(0.1, 1.0, 10)
)

# Calculate the mean and standard deviation of the training scores
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)

# Calculate the mean and standard deviation of the test scores
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure()
plt.plot(train_sizes, train_scores_mean, label='Training Score')
plt.plot(train_sizes, test_scores_mean, label='Cross-Validation Score')
plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1)
plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1)
plt.xlabel('Training Set Size')
plt.ylabel('Score')
plt.title('Learning Curve - AdaBoost Classifier')
plt.legend()
plt.show()

# # Train the Random Forest classifier with the best hyperparameters
# b_abc.fit(x_train_20_res, y_train_20_res)

# Get feature importances
feature_importances = b_abc.feature_importances_

# Sort feature importances in descending order
sorted_indices = np.argsort(feature_importances)[::-1]

# Get the names of the features
feature_names = x_train_20_res.columns

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.barh(range(len(feature_importances)), feature_importances[sorted_indices][::-1])
plt.yticks(range(len(feature_importances)), feature_names[sorted_indices][::-1])
plt.xlabel('Importance')
plt.ylabel('Features')
plt.title('Feature Importance - AdaBoost Classifier')
plt.tight_layout()
plt.show()

"""##### 3.6.2 Random Forest Hyperparameter Tuning"""

rfc = RandomForestClassifier()

# define the grid of values to search
grid = dict()
grid['n_estimators'] = [10, 50, 100, 500]
grid['max_features'] = ['sqrt', 'log2', None]
grid['max_depth'] = [9, 12, 15]
grid['max_leaf_nodes'] = [9, 12, 15]

# define the rs search procedure
rs_search = RandomizedSearchCV(estimator=rfc, param_distributions=grid, n_jobs=-1, cv=5, scoring='f1')

# execute the rs search
rs_result = rs_search.fit(x_train_20_res, y_train_20_res)

# summarize the best score and configuration
print("Best: %f using %s" % (rs_result.best_score_, rs_result.best_params_))

# input best_params_ to new classifier
rfc = RandomForestClassifier(
                            n_estimators=500,
                            max_features='sqrt',
                            max_depth=12,
                            max_leaf_nodes=15
                            )

# fit pipeline to data train
rfc.fit(x_train_20_res, y_train_20_res)

# evaluate model
y_train_pred = rfc.predict(x_train_20_res)
y_pred = rfc.predict(x_test_20)

from sklearn.metrics._classification import classification_report

target_names = ['good_loan', 'bad_loan']

print(classification_report(y_test, y_pred, digits=3, target_names= target_names))

c_matrix_plot(y_test, y_pred)

"""### Sources

1. https://www.researchgate.net/publication/327787704/figure/tbl1/AS:673118972559377@1537495477959/Loan-statuses-in-LendingClub.png
2. http://kb.lendingclub.com/investor/articles/Investor/What-do-the-different-Note-statuses-mean/
3. https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/#h-smote-synthetic-minority-oversampling-technique
"""